{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d5e3bc-35f7-43fd-a34d-5d94a21ba236",
   "metadata": {},
   "source": [
    "# DESCRIPTION\n",
    "This notebook reads in the data and runs the \"monthly avg\" lasso regression for each grid cell. The notebook is designed to be executed with 10 processors as it decomposes the 2D domain in the x-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f45393-1eba-4fa0-a279-258e30248610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyproj\n",
    "import rioxarray as rxr\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    INPUTS\n",
    "\"\"\"\n",
    "me = 0\n",
    "num_proc = 10\n",
    "num_grid = 3\n",
    "tot_grid = (num_grid * 2) + 1\n",
    "\n",
    "print(f'processor : {me}')\n",
    "print(f'tot_grid : {tot_grid * tot_grid}')\n",
    "\n",
    "fpath_out = '/glade/scratch/rossamower/snow/snowmodel/aso/california/tuolumne/snowmodel/cheyenne/outputs/aso_sm/ml_results/grid/lasso/avg/7_7/'\n",
    "\n",
    "\"\"\"\n",
    "    LOAD DATASET\n",
    "\"\"\"\n",
    "ds_output_dir = '/glade/scratch/rossamower/snow/snowmodel/aso/california/tuolumne/snowmodel/cheyenne/outputs/aso_sm/'\n",
    "ds = xr.load_dataset(ds_output_dir + 'aso_sm_merge_clean_2.nc')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(df,testyr,trainyr,isOneHot = True):\n",
    "    ## one hot encoding based on month ##\n",
    "    totyrs = trainyr + testyr\n",
    "    if isOneHot == True:\n",
    "        df_totyrs = df[df.date_year.isin(totyrs)]\n",
    "        if 2015 in testyr:\n",
    "            df3 = df_totyrs[df_totyrs.date_month != 2]\n",
    "            df3 = df3[df3.date_month != 7]\n",
    "        elif 2017 in testyr:\n",
    "            df3 = df_totyrs[df_totyrs.date_month != 2]\n",
    "            df3 = df3[df3.date_month != 1]\n",
    "            df3 = df3[df3.date_month != 8]\n",
    "            df3 = df3[df3.date_month != 4]\n",
    "        else:\n",
    "            df3 = df_totyrs[df_totyrs.date_month != 2]\n",
    "            df3 = df3[df3.date_month != 1]\n",
    "            df3 = df3[df3.date_month != 8]\n",
    "        df_hot1 = pd.get_dummies(data=df3, columns=['date_month'],drop_first = False)\n",
    "    else:\n",
    "        df_hot1 = df\n",
    "        \n",
    "    ## train/test ##\n",
    "    df_train = df_hot1[df_hot1.date_year.isin(trainyr)]\n",
    "    df_test = df_hot1[df_hot1.date_year.isin(testyr)]\n",
    "    ## drop na's by columns##\n",
    "    df_tn_nona = df_train.dropna(axis = 1, how = 'all')\n",
    "    df_ts_nona = df_test.dropna(axis = 1, how = 'all')\n",
    "    ## drop na's  by rows##\n",
    "    df_tn_nona = df_tn_nona.dropna(axis = 0, how = 'any')\n",
    "    df_ts_nona = df_ts_nona.dropna(axis = 0, how = 'any')\n",
    "    ## create index vector ##\n",
    "    index_ = df_ts_nona.index\n",
    "    ## index of columns ##\n",
    "    \n",
    "    ## pull out labels ##\n",
    "    y_train_ = df_tn_nona.aso_swe\n",
    "    y_test_ = df_ts_nona.aso_swe\n",
    "    ## drop columns ##\n",
    "    if isOneHot == True:\n",
    "        df_tn_nona_ = df_tn_nona.drop(columns = ['aso_swe','date_year'])\n",
    "        df_ts_nona_ = df_ts_nona.drop(columns = ['aso_swe','date_year'])\n",
    "    else:\n",
    "        df_tn_nona_ = df_tn_nona.drop(columns = ['aso_swe','date_year','date_month'])\n",
    "        df_ts_nona_ = df_ts_nona.drop(columns = ['aso_swe','date_year','date_month'])\n",
    "    \n",
    "    return df_tn_nona_,y_train_,df_ts_nona_,y_test_,index_,df_test\n",
    "    \n",
    "    \n",
    "def reg_stats(y,yhat,X):\n",
    "    SS_Residual = sum((y-yhat)**2)       \n",
    "    SS_Total = sum((y-np.mean(y))**2) \n",
    "    if SS_Total == 0.0:\n",
    "        r_squared = 0.0\n",
    "    else:\n",
    "        r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "    if len(y)-X.shape[1]-1 == 0:\n",
    "        adjusted_r_squared = -1.0\n",
    "    else:\n",
    "        adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "    \n",
    "    return r_squared,adjusted_r_squared\n",
    "    \n",
    "    \n",
    "\n",
    "def lr1(X,y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def log_inverse(arry):\n",
    "    arry =  np.exp(arry) - 1\n",
    "    return arry\n",
    "    \n",
    "    \n",
    "def lasso_best_cell(x_train,y_train,isOneHot):\n",
    "    r2_dict = {}\n",
    "    r2adj_dict = {}\n",
    "    sm_string = 'sm_'\n",
    "    ## get column names ##\n",
    "    col_names = x_train.columns.values.tolist()\n",
    "    ## get get grid indexes ##\n",
    "    sm_list = [ x for x in col_names if sm_string in x ]\n",
    "    index_int = [ int(x.replace(sm_string,'')) for x in sm_list ]\n",
    "    \n",
    "    ## loop through adjacent grid cells and compare to aso ##\n",
    "    for score in range(0,x_train.shape[1]):\n",
    "        date_lst = [ x for x in col_names if \"date_month\" in x ]\n",
    "        # fit #\n",
    "        if isOneHot == False:\n",
    "            x_train_np = x_train.iloc[:,score].values\n",
    "            model_ = lr1(x_train_np.reshape(-1,1),y_train)\n",
    "        # predict #\n",
    "            baseline_yhat_tf = model_.predict(x_train_np.reshape(-1,1))\n",
    "            r_squared,adjusted_r_squared = reg_stats(y_train,baseline_yhat_tf,x_train_np.reshape(-1,1))\n",
    "        else:\n",
    "            date_lst = [ x for x in col_names if \"date_month\" in x ]\n",
    "            date_lst.append(f'sm_{score}')\n",
    "            x_train_trim = x_train[date_lst]\n",
    "            model_ = lr1(x_train_trim,y_train)\n",
    "            baseline_yhat_tf = model_.predict(y_train.values.reshape(-1,1))\n",
    "            \n",
    "        # stats #\n",
    "        # r_squared,adjusted_r_squared = reg_stats(y_train.values,baseline_yhat_tf,x_train)\n",
    "        \n",
    "        r2_dict[index_int[score]] = [[r_squared,adjusted_r_squared]]\n",
    "        r2adj_dict[index_int[score]] = adjusted_r_squared\n",
    "    \n",
    "    idx_r2 = max(r2_dict,key=r2_dict.get)\n",
    "    max_r2adj = max(r2adj_dict.values())\n",
    "    return idx_r2,max_r2adj,r2_dict,r2adj_dict,index_int\n",
    "    \n",
    "    \n",
    "def lasso_loop2(r2_dict,idx_r2,x_train,y_train):\n",
    "    \n",
    "    interim_dict_r2 = {}\n",
    "    interim_dict_r2adj = {}\n",
    "    sm_string = 'sm_'\n",
    "    for k,v in r2_dict.items():\n",
    "        if sm_string+str(k) in idx_r2:\n",
    "            interim_dict_r2[k] = -0.0\n",
    "            interim_dict_r2adj[k] = -0.0\n",
    "            r2_dict[k].append([-0.0,-0.0])\n",
    "        else:\n",
    "            lst_ = idx_r2.copy()\n",
    "            lst_.append(sm_string + str(k))\n",
    "            x_train_ = x_train.loc[:, lst_]\n",
    "            # r2__, r2adj__,model_,yhat = lr1(x_train_,y_train)   \n",
    "            model_ = lr1(x_train_,y_train)\n",
    "            yhat = model_.predict(x_train_)\n",
    "            r2__,r2adj__ = reg_stats(y_train,yhat,x_train_)\n",
    "            \n",
    "            r2_dict[k].append([r2__,r2adj__])\n",
    "            interim_dict_r2[k] = r2__\n",
    "            interim_dict_r2adj[k] = r2adj__\n",
    "    idx_r2_ = max(r2_dict,key=interim_dict_r2.get)\n",
    "    max_r2adj_ = max(interim_dict_r2adj.values())\n",
    "    return idx_r2_,max_r2adj_,r2__,r2_dict,model_,yhat\n",
    "    \n",
    "    \n",
    "def lasso_loop(idx_r2,x_train,y_train,max_r2adj,r2_dict,isOneHot):\n",
    "    flag = True\n",
    "    count = 0\n",
    "    blah = []\n",
    "    sm_string = 'sm_'\n",
    "    while flag == True:\n",
    "        if count == 0:\n",
    "            blah.append(sm_string + str(idx_r2))\n",
    "        r2_idx, max_r2adj_,r2_,r2_dict,model_ls,yhat = lasso_loop2(r2_dict,blah,x_train,y_train)\n",
    "        count += 1\n",
    "        if (max_r2adj_ < max_r2adj):\n",
    "            flag = False\n",
    "        else:\n",
    "            max_r2adj = max_r2adj_\n",
    "            blah.append(sm_string + str(r2_idx))\n",
    "            if (count > x_train.shape[1]-2):\n",
    "                flag = False\n",
    "    index_int = [ int(x.replace(sm_string,'')) for x in blah ]\n",
    "                \n",
    "    return index_int,blah\n",
    "    \n",
    "    \n",
    "def grid_lr(df,testyr,trainyr,isOneHot,hasKeys=None):\n",
    "    \n",
    "    sm_string = 'sm_'\n",
    "    x_train, y_train, x_test, y_test, index,df_test1 = preprocess(df,testyr,trainyr,isOneHot)\n",
    "    \n",
    "    if hasKeys is None:\n",
    "        idx_r2,max_r2adj,r2_dict,r2adj_dict,nonna_indexes = lasso_best_cell(x_train,y_train,isOneHot)\n",
    "        if max(r2_dict.values())[0][0] <= 0.0:\n",
    "            chosen_indexes = [idx_r2]\n",
    "            chosen_strings = [ sm_string + str(x) for x in chosen_indexes ]\n",
    "        else:\n",
    "            chosen_indexes,chosen_strings = lasso_loop(idx_r2,x_train,y_train,max_r2adj,r2_dict,isOneHot)\n",
    "    else:\n",
    "        chosen_indexes = hasKeys\n",
    "        chosen_strings = [ sm_string + str(x) for x in chosen_indexes ]\n",
    "        \n",
    "    if x_train.loc[:,chosen_strings].shape[1] == 1:\n",
    "    ## fit ##\n",
    "        model = lr1(x_train.loc[:,chosen_strings].values.reshape(-1,1),y_train)\n",
    "    ## predictions ##\n",
    "        baseline_yhat_tf = model.predict(x_test.loc[:,chosen_strings].values.reshape(-1,1))\n",
    "    else:\n",
    "    ## fit ##\n",
    "        model = lr1(x_train.loc[:,chosen_strings],y_train)\n",
    "    ## predictions ##\n",
    "        baseline_yhat_tf = model.predict(x_test.loc[:,chosen_strings])\n",
    "    \n",
    "    ## get stats ##\n",
    "    r_squared,adjusted_r_squared = reg_stats(y_test,baseline_yhat_tf,x_test.loc[:,chosen_strings])\n",
    "\n",
    "    ## model parameters ##\n",
    "    bias = model.intercept_\n",
    "    coef = model.coef_\n",
    "    yhat_bl = baseline_yhat_tf.flatten()\n",
    "    \n",
    "    ## log inverse ##\n",
    "    yhat_bl = log_inverse(yhat_bl)\n",
    "    \n",
    "    \n",
    "    return df_test1,index,yhat_bl,bias,coef,r_squared,chosen_indexes\n",
    "    \n",
    "    \n",
    "def data_merge(pred,name,index,df_orig,lst):\n",
    "    \n",
    "    result = pd.DataFrame(data = {name:pred},\n",
    "                              index = index)\n",
    "    \n",
    "    df_1 = pd.merge(df_orig, result, \n",
    "                    left_index = True, right_index = True, \n",
    "                    how=\"left\", indicator=False)\n",
    "    \n",
    "    lst.append(df_1)\n",
    "    \n",
    "    return lst\n",
    "            \n",
    "    \n",
    "def common_keys(idx_lst):\n",
    "    idx_dic = {}\n",
    "    for k in idx_lst:\n",
    "        for item in k:\n",
    "            if item in idx_dic.keys():\n",
    "                idx_dic[item] += 1\n",
    "            else:\n",
    "                idx_dic[item] = 1\n",
    "    itemMaxValue = max(idx_dic.items(), key=lambda x: x[1])\n",
    "    listOfKeys = list()\n",
    "    # Iterate over all the items in dictionary to find keys with max value\n",
    "    counter = 0\n",
    "    for key, value in idx_dic.items():\n",
    "        if value == itemMaxValue[1]:\n",
    "            listOfKeys.append(key)\n",
    "            counter += 1\n",
    "    if counter == len(idx_dic):\n",
    "        listOfKeys = idx_lst[-1]      \n",
    "    return listOfKeys\n",
    "    \n",
    "    \n",
    "def reindex(index,tot_grid,num_grid,i,j):\n",
    "    i_index = []\n",
    "    j_index = []\n",
    "    for k in index:\n",
    "        ct = 0\n",
    "        for row in range(tot_grid):\n",
    "            for col in range(tot_grid):\n",
    "                if ct == k:\n",
    "                    idx = row - num_grid\n",
    "                    idy = col - num_grid\n",
    "                ct += 1\n",
    "\n",
    "        new_i = i + idx\n",
    "        new_j = j + idy\n",
    "        i_index.append(new_i)\n",
    "        j_index.append(new_j)\n",
    "\n",
    "    \n",
    "    return j_index,i_index\n",
    "    \n",
    "def pad_var(nparry,tot_grid):\n",
    "    return np.pad(nparry, (0,(tot_grid*tot_grid)-len(nparry),), 'constant',constant_values= -9999.0)\n",
    "    \n",
    "    \n",
    "def create_dataframe(X,y,date):\n",
    "    \n",
    "    df = pd.DataFrame({'date_t':date,\n",
    "            'aso_swe':y}) \n",
    "    \n",
    "    for col in range(0,X.shape[1]):\n",
    "            df[f'sm_{col}'] = X[:,col]\n",
    "            \n",
    "    col_names = df.columns.values.tolist()\n",
    "    \n",
    "    col_names.remove(\"date_t\")\n",
    "    \n",
    "    df['month_year'] = df['date_t'].dt.to_period('M')\n",
    "    df_gp = pd.DataFrame(df.groupby('month_year')[col_names].mean())\n",
    "    df_gp = df_gp.reset_index()\n",
    "    df_gp['date_year'] = df_gp['month_year'].dt.year\n",
    "    df_gp['date_month'] = df_gp['month_year'].dt.month\n",
    "    df_gp = df_gp.drop(columns = ['month_year'])\n",
    "    \n",
    "    return df_gp\n",
    "\n",
    "\n",
    "def colin(df,testyr,trainyr,isOneHot,hasKeys=None):\n",
    "\n",
    "    \n",
    "    sm_string = 'sm_'\n",
    "\n",
    "    x_train, y_train, x_test, y_test, index,df_test1 = preprocess(df,testyr,trainyr,isOneHot)\n",
    "\n",
    "    \n",
    "    chosen_indexes = hasKeys\n",
    "    chosen_strings = [ sm_string + str(x) for x in chosen_indexes ]\n",
    "\n",
    "    counter = 0\n",
    "    str_lst = []\n",
    "    multFlag = True\n",
    "    while multFlag == True:\n",
    "\n",
    "        str_lst.append(chosen_strings[counter])\n",
    "        chosen_pd_cols = str_lst\n",
    "\n",
    "        x_train_ = x_train.loc[:,chosen_pd_cols]\n",
    "        X_constant = sm.add_constant(x_train_)\n",
    "        vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "        vif_df = pd.DataFrame({'vif': vif}, index=X_constant.columns).T\n",
    "\n",
    "        if vif_df[chosen_strings[counter]].values[0] > 5:\n",
    "            multFlag = False\n",
    "            str_lst.remove(chosen_strings[counter])\n",
    "        else:\n",
    "            if (counter+1 == len(chosen_strings)):\n",
    "                multFlag = False\n",
    "        counter += 1\n",
    "\n",
    "        \n",
    "    \n",
    "    chosen_pd_cols = str_lst\n",
    "    ## fit ##\n",
    "    model = lr1(x_train.loc[:,chosen_pd_cols],y_train)\n",
    "    ## predictions ##\n",
    "    baseline_yhat_tf = model.predict(x_test.loc[:,chosen_pd_cols])\n",
    "    counter += 1\n",
    "    \n",
    "    ## get stats ##\n",
    "    r_squared,adjusted_r_squared = reg_stats(y_test,baseline_yhat_tf,x_test.loc[:,chosen_pd_cols])\n",
    "\n",
    "    ## model parameters ##\n",
    "    bias = model.intercept_\n",
    "    coef = model.coef_\n",
    "    yhat_bl = baseline_yhat_tf.flatten()\n",
    "    \n",
    "    ## log inverse ##\n",
    "    yhat_bl = log_inverse(yhat_bl)\n",
    "    \n",
    "    index_int = [ int(x.replace(sm_string,'')) for x in str_lst ]\n",
    "    \n",
    "    \n",
    "    return df_test1,index,yhat_bl,bias,coef,r_squared,index_int\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def run_grid(ds,i,j,y_val,num_grid,tot_grid):\n",
    "        ## pull out values #\n",
    "        x_val = np.log1p(ds.sm_swe[:,j-num_grid:j+num_grid+1,i-num_grid:i+num_grid+1].values,dtype=np.float128)\n",
    "        rshp_x = x_val.reshape(ds.time.shape[0],tot_grid*tot_grid,order = 'F')\n",
    "        date_t = np.array(pd.to_datetime(ds.time[:].values))\n",
    "        date_month = np.array(pd.to_datetime(ds.time[:].values).month)\n",
    "        date_year = np.array(pd.to_datetime(ds.time[:].values).year) \n",
    "        \n",
    "        \n",
    "            \n",
    "        df_lst_bl = []\n",
    "        \n",
    "\n",
    "        df1 = create_dataframe(rshp_x,y_val,date_t)\n",
    "        \n",
    "        yr_list = [[2015,2016],[2017,2018]]\n",
    "\n",
    "        yr_test = [2019,2020]\n",
    "        training_yrs = [2013,2014]\n",
    "\n",
    "        bias_bl_lst = []\n",
    "        r2_bl_lst = []\n",
    "        chosen_index_lst = []\n",
    "        \n",
    "        for yr in yr_list:\n",
    "            testing_yrs = yr\n",
    "            ## create dataframe for grid ##\n",
    "            isOneHot = False\n",
    "            df_test1,index,yhat_bl,bias_bl_tn,coef_bl_tn,r2_bl_tn,chosen_indexes = grid_lr(df1,testing_yrs,\n",
    "                                             training_yrs,isOneHot,hasKeys=None)\n",
    "            \n",
    "            chosen_index_lst.append(chosen_indexes)\n",
    "            \n",
    "            \n",
    "            df_lst_bl = data_merge(yhat_bl,'yhat_bl',index,df_test1,df_lst_bl)\n",
    "            \n",
    "            \n",
    "            training_yrs.append(yr[0])\n",
    "            training_yrs.append(yr[1])\n",
    "            \n",
    "            bias_bl_lst.append(bias_bl_tn)\n",
    "            \n",
    "            r2_bl_lst.append(r2_bl_tn)\n",
    "            \n",
    "        ## run last training ##  \n",
    "        key_list = common_keys(chosen_index_lst)\n",
    "            \n",
    "        isOneHot = False\n",
    "        df_test1,index,yhat_bl,bias_bl_ts,coef_bl_ts,r2_bl_ts,final_index = colin(df1,yr_test,\n",
    "                                 training_yrs,isOneHot,key_list)\n",
    "        \n",
    "        final_index_j, final_index_i = reindex(final_index,tot_grid,num_grid,i,j)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_lst_bl = data_merge(yhat_bl,'yhat_bl',index,df_test1,df_lst_bl)\n",
    "        \n",
    "        \n",
    "        bias_bl_lst.append(bias_bl_ts)\n",
    "        \n",
    "        r2_bl_lst.append(r2_bl_ts)\n",
    "        \n",
    "        df_2_bl = pd.concat(df_lst_bl)\n",
    "        \n",
    "        df_3_bl = df_2_bl.fillna(-9999.0)\n",
    "        \n",
    "        ## pad arrays that can vary in size ##\n",
    "        coef_bl_ts = pad_var(coef_bl_ts,tot_grid)\n",
    "        final_index_j = pad_var(np.array(final_index_j),tot_grid)\n",
    "        final_index_i = pad_var(np.array(final_index_i),tot_grid)\n",
    "        \n",
    "        return bias_bl_lst,r2_bl_lst,list(coef_bl_ts),df_3_bl,final_index,list(final_index_j),list(final_index_i)\n",
    "        \n",
    "        \n",
    "def lst_to_output(lst,name,fpath,me,toFile=True):\n",
    "    arr = np.array(lst,dtype = np.float64)\n",
    "    arr[arr == -9999.0] = np.nan\n",
    "    if toFile == True:\n",
    "        arr.tofile(fpath + name +'_ln_'  + str(me) +'_.gdat')\n",
    "    return arr\n",
    "    \n",
    "\"\"\"\n",
    "    RUN MODEL\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "start = time.time()\n",
    "import sys \n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.01 \n",
    "\n",
    "if me == 0:\n",
    "    i_start = 0\n",
    "    i_end = i_start + 101\n",
    "elif me == num_proc:\n",
    "    i_start = (me * 100) + 1\n",
    "    i_end = i_start + 45\n",
    "else:\n",
    "    i_start = (me * 100) + 1\n",
    "    i_end = i_start + 100\n",
    "    \n",
    "\n",
    "bl_lst = []\n",
    "bl_r2_lst = []\n",
    "bl_bias_lst = []\n",
    "bl_coef_lst = []\n",
    "bl_i_lst = []\n",
    "bl_j_lst = []\n",
    "\n",
    "\n",
    "\n",
    "## create pd datatime that is grouped by year and month to join data to ##\n",
    "date_pd = pd.DataFrame({'time':pd.to_datetime(ds.time[:].values)})#,\n",
    "date_pd['month_year'] = date_pd['time'].dt.to_period('M')\n",
    "date_pd['blank'] = 1\n",
    "df_new = pd.DataFrame(date_pd.groupby('month_year')['blank'].mean())\n",
    "df_new = df_new.reset_index()\n",
    "\n",
    "for i in range(i_start,i_end):\n",
    "    print(i,end = ' ')\n",
    "    for j in range(ds.y.shape[0]):\n",
    "        if np.isnan(ds.notGrid[j,i].values) == True: # dont run model \n",
    "            yhat_bl = list(np.full((23), -9999.0))\n",
    "            r2_bl = list(np.full((3), -9999.0))\n",
    "            coef_bl = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "            final_index_j = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "            final_index_i = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "            bias_bl = list(np.full((3), -9999.0))\n",
    "            \n",
    "        else:\n",
    "            y_val = np.log1p(ds.aso_swe[:,j,i].values,dtype=np.float128)\n",
    "        ## grid regression ##\n",
    "            try:\n",
    "                bias_bl,r2_bl,coef_bl,df_bl,final_index,final_index_j,final_index_i = run_grid(ds,i,j,y_val,num_grid,tot_grid)\n",
    "                if r2_bl[-1] < 0.0:\n",
    "                    yhat_bl = list(np.full((23), -9999.0))\n",
    "                    r2_bl = list(np.full((3), -9999.0))\n",
    "                    coef_bl = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "                    bias_bl = list(np.full((3), -9999.0))\n",
    "                    final_index_j = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "                    final_index_i = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "                else:\n",
    "                    df_bl_j = df_new.join(df_bl,how = 'outer')\n",
    "                    df_bl_j = df_bl_j[df_bl_j['sm_0'].notna()]\n",
    "                    df_bl_j = df_bl_j.fillna(-9999.0)\n",
    "                    yhat_bl = df_bl_j['yhat_bl'].to_list() #yhat_oh\n",
    "\n",
    "            except:\n",
    "                yhat_bl = list(np.full((23), -9999.0))\n",
    "                r2_bl = list(np.full((3), -9999.0))\n",
    "                coef_bl = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "                final_index_j = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "                final_index_i = list(np.full((tot_grid*tot_grid), -9999.0))\n",
    "                bias_bl = list(np.full((3), -9999.0))\n",
    "\n",
    "        for val in range(0,len(yhat_bl)):\n",
    "            bl_lst.append(yhat_bl[val])\n",
    "            \n",
    "        for val in range(0,len(bias_bl)):\n",
    "            bl_bias_lst.append(bias_bl[val])\n",
    "            bl_r2_lst.append(r2_bl[val])\n",
    "            \n",
    "        for val in range(0,tot_grid*tot_grid):\n",
    "            bl_coef_lst.append(coef_bl[val])\n",
    "            bl_i_lst.append(final_index_i[val])\n",
    "            bl_j_lst.append(final_index_j[val])\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "                \n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "                \n",
    "        \n",
    "bl_arr = lst_to_output(bl_lst,'avg_yhat',fpath_out,me,toFile=True)   \n",
    "bl_r2_arr = lst_to_output(bl_r2_lst,'avg_r2',fpath_out,me,toFile=True)  \n",
    "bl_bias_arr = lst_to_output(bl_bias_lst,'avg_bias',fpath_out,me,toFile=True)  \n",
    "bl_coef_arr = lst_to_output(bl_coef_lst,'avg_coef',fpath_out,me,toFile=True)\n",
    "bl_i_arr = lst_to_output(bl_i_lst,'i_index',fpath_out,me,toFile=True)  \n",
    "bl_j_arr = lst_to_output(bl_j_lst,'j_index',fpath_out,me,toFile=True)\n",
    " \n",
    "\n",
    "\n",
    "print('END LINEAR REGRESSION ---------------->')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucb_w207",
   "language": "python",
   "name": "ucb_w207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
